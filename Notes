sed -i 's/old-text/new-text/g' input.txt

redis.yml

apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: redis
    type: backend
spec:
  containers:
    - name: redis
      image: redis
---------------------------------

cat replicaset-definition-1.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
  labels:
        tier: frontend
spec:
  template:
    metadata:
      name: app1
      labels:
        app: app1
        type: front-end
    spec:
      containers:
      - name: nginx
        image: nginx
  replicas: 2
  selector:
    matchLabels:
      type: front-end

-------------------------------------

 cat replicaset-definition-2.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-2
  labels:
        tier: front-end
spec:
  template:
    metadata:
      name: app2
      labels:
        app: app2
        type: front-end
    spec:
      containers:
      - name: nginx
        image: nginx
  replicas: 2
  selector:
    matchLabels:
      type: front-end


kubectl scale --replicas=2 replicaset new-replica-set 

------------------------

kubectl get all

deployment has same definition as replicaset except for kind. 

Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginxla

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create -f nginx-deployment.yaml

OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

---------------

 cat deployment-definition-1.yaml 

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
  labels:
    app: deployment-1
    type: front-end
spec:
  template:
    metadata:
      name: deployment-1
      labels: 
        app: deployment-1 
        type: front-end
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600
  replicas: 4
  selector:
    matchLabels:
      type: front-end

---------
services:

* node port
* cluster IP
* load balancer

node port

apiVersion: v1
kind: Service
metadata:
    name: myapp-service

spec:
     type: NodePort
     ports:
      - targetPort: 80
        port: 80
        nodePort: 30008

     selector:
        app: myapp
        type: front-end


--------------

it could be three pods on the same node or three pods on three different nodes.

target port of pod :80
port  of service : 80
node port : 30008

-----------------------------------------

Cluster IP - service-definition.yml 

apiVersion: v1
kind: Service
metadata:
    name: back-end
spec:
   type: ClusterIP
   ports:
   - targetPort: 80
     port: 80
   selector:
     app: myapp
     type: back-end

------------------------

Load Balancer - service-definition.yml 

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
spec:
   type: LoadBalancer
   ports:
   - targetPort: 80
     port: 80
     nodePort: 30008

------------------------------

creating a node port service

controlplane ~ ➜  cat service-definition-1.yaml 

---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp

--------------------

namespaces 

namespace-dev.yml

apiVersion: v1
kind: Namespace
metadata: 
    name: dev

--------------------------
kubectl create -f namespace-dev.yml
kubectl create namespace dev

kubectl config set-context $(kubectl config current-context) --namespace dev
kubectl get pods --all-namespaces 

--------------------------------

Resource quota

apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

---------

Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port.
 I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

Reference:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

https://kubernetes.io/docs/reference/kubectl/conventions/=


--------------------


kubectl get pods --selector=tier=frontend --selector=bu
kubectl get pods --selector env=prod,bu=finance,tier=frontend


controlplane ~ ➜  cat replicaset-definition-1.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: front-end
   template:
     metadata:
       labels:
        tier: front-end 
     spec:
       containers:
       - name: nginx
         image: nginx

------

taints and tolerations

kubectl taint nodes node-name key=value:taint-effect [noSchedule,PreferNoSchedule,NoExecute]

pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: app
    operator: "Equal"
    value: "Blue"
    effect: "NoSchedule"


kubectl describe node kubemaster | grep Taint
kubectl taint nodes node01 spray=mortein:NoSchedul



bee.yaml

controlplane ~ ➜  cat bee.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  tolerations: 
  - key: spray
    operator: "Equal"
    value: "mortein"
    effect: "NoSchedule"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

------


add a hyphen to remove taint from node
kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

-------------

node selector 

kubectl label nodes node-name label-key=label-value
kubectl label nodes node-1 size=Large

its limited so we use node affinity

node affinity types

Available
1.requiredDuringSchedulingIgnoredDuringExecution
2.preferredDuringSchedulingIgnoredDuringExecution

Planned:
1.requiredDuringSchedulingRequiredDuringExecution



Name: blue

Replicas: 3

Image: nginx

NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

Key: color

value: blue



spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue



Name: red

Replicas: 2

Image: nginx

NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

Key: node-role.kubernetes.io/control-plane

Use the right operator


spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - controlplane

----------


n the previous lecture, I said - "When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi".
 For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.



apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/



apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/



References:

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource

-----

practice memory and cpu limits on 
https://killercoda.com/playgrounds/scenario/cka
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

kubectl create namespace default-cpu-example

Limit Range example


apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container


kubectl create -f limit-range.yaml --namespace=default-cpu-example
kubectl get limitrange -n default-cpu-example

Now if you create a Pod in the default-cpu-example namespace, and any container in that Pod does not specify its own values for CPU request and CPU limit,
 then the control plane applies default values: a CPU request of 0.5 and a default CPU limit of 1.

Here's a manifest for a Pod that has one container. The container does not specify a CPU request and limit.





cpu-defauls-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx


kubectl create -f cpu-default-pod.yaml --namespace default-cpu-example
kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example

The output shows that the Pod's only container has a CPU request of 500m cpu (which you can read as “500 millicpu”), and a CPU limit of 1 cpu.
 These are the default values specified by the LimitRange.


What if you specify a container's limit, but not its request? 

cpu-default-pod-2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-2
spec:
  containers:
  - name: default-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "1"


The output shows that the container's CPU request is set to match its CPU limit. Notice that the container was not assigned the default CPU request value of 0.5 cpu:

--> 

resources:
      limits:
        cpu: "1"
      requests:
        cpu: "1"


What if you specify a container's request, but not its limit?

cpu-default-pod-3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-3
spec:
  containers:
  - name: default-cpu-demo-3-ctr
    image: nginx
    resources:
      requests:
        cpu: "0.75"

The output shows that the container's CPU request is set to the value you specified at the time you created the Pod (in other words: it matches the manifest). 
However, the same container's CPU limit is set to 1 cpu, which is the default CPU limit for that namespace.

resources:
  limits:
    cpu: "1"
  requests:
    cpu: 750m

----------------------------------------------------

kubectl create namespace default-mem-example


memory-default.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

kubectl create -f memory-default.yaml 


Now if you create a Pod in the default-mem-example namespace, and any container within that Pod does not specify its own values for memory request and memory limit,
then the control plane applies default values: a memory request of 256MiB and a memory limit of 512MiB.

apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo
spec:
  containers:
  - name: default-mem-demo-ctr
    image: nginx

kubectl create -f default-memory-pod.yaml

kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example

The output shows that the Pod's container has a memory request of 256 MiB and a memory limit of 512 MiB. These are the default values specified by the LimitRange.

-->

resources:
      limits:
        memory: 512Mi
      requests:
        memory: 256Mi


What if you specify a container's limit, but not its request?

default-memory-pod-2.yaml


apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-2
spec:
  containers:
  - name: default-mem-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "1Gi"


kubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example


The output shows that the container's memory request is set to match its memory limit. 
Notice that the container was not assigned the default memory request value of 256Mi.

-->

resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi

----------------------

What if you specify a container's request, but not its limit?

default-memory-pod-3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-3
spec:
  containers:
  - name: default-mem-demo-3-ctr
    image: nginx
    resources:
      requests:

        memory: "128Mi"


The output shows that the container's memory request is set to the value specified in the container's manifest. The container is limited to use no more than 512MiB of memory, which matches the default memory limit for the namespace.

resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi

--------------------------------

Daemon Sets -- 


apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
     matchLabels:
	app: monitoring-agent
  template:
     metadata:
       labels: 
         app: monitoring-agent
     spec:
       containers:
       - name: monitoring-agent
         image: monitoring-agent


Deploy a DaemonSet for FluentD Logging.

Use the given specifications.

CheckCompleteIncomplete
Name: elasticsearch

Namespace: kube-system

Image: k8s.gcr.io/fluentd-elasticsearch:1.20


An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command 
kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml. 
Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet.

Finally, create the Daemonset by running kubectl create -f fluentd.yaml


controlplane ~ ➜  cat fluentd.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  creationTimestamp: null
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        resources: {}

-------------------------

Static pods

kubelet -->  /etc/kubernetes/manifests
kubelet.service --> --pod-manifest-path = /etc/kubernetes/manfiests
or in  		    --config=kubeconfig.yaml    -->  inside this file --> staticPodPath: /etc/kubernetes/manifests
 
you cant modify static pod while running you have to edit manifest file

Static Pods Vs Daemon Sets

Static Pods --> created by Kubelet.   --> Deploy Control Plane Components as Static Pods
DaemonSets --> Created by Kube-API-Server ( DaemonSet Controller) --> Deploy Monitoring Agents, Logging Agents on nodes

Both ignored by Kube Scheduler. 

----------------------------

Find static pods

Run the command kubectl get pods --all-namespaces and look for those with -controlplane appended in the name

Run the kubectl get pods --all-namespaces -o wide and identify the node in which static pods are deployed.

Create a static pod named static-busybox that uses the busybox image and the command sleep 1000 

-->   Create a pod definition file called static-busybox.yaml with the provided specs and place it under /etc/kubernetes/manifests directory

kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml


edit to busybox:1.28.4

Simply edit the static pod definition file and save it. If that does not re-create the pod, run: kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

---------------------------

multiple schedulers

- default scheduler 

my-scheduler-config.yaml

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler


kubectl get events -o wide
kubectl logs my-custom-scheduler --name-space=kube-system


We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.


Checkout the following Kubernetes objects:

ServiceAccount: my-scheduler (kube-system namespace)
ClusterRoleBinding: my-scheduler-as-kube-scheduler
ClusterRoleBinding: my-scheduler-as-volume-scheduler

Run the command: kubectl get serviceaccount -n kube-system & kubectl get clusterrolebinding

Note: - Don't worry if you are not familiar with these resources. We will cover it later o


Let's create a configmap that the new scheduler will employ using the concept of ConfigMap as a volume.
Create a configmap with name my-scheduler-config using the content of file /root/my-scheduler-config.yaml

kubectl create cm my-scheduler-config -n kube-system --from-file /root/my-scheduler-config.yaml 



Deploy an additional scheduler to the cluster following the given specification.

Use the manifest file provided at /root/my-scheduler.yaml. Use the same image as used by the default kubernetes scheduler  (k8s.gcr.io/kube-scheduler:v1.24.0)

--> modify yaml file with correct image then

kubectl create -f /root/my-scheduler -n kube-system


A POD definition file is given. Use it to create a POD with the new custom scheduler.

File is located at /root/nginx-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  schedulerName: my-scheduler
                       

-----------------------------------------------

Scheduling profiles

pod-definition.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  priorityClassName: high-priority 
  containers: 
  - name: simple-webapp-color
    image: simple-webapp-color
    resources:
      requests:
        memory: "1Gi"
	cpu: 10


apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 10000000
globalDefault: false
description: " This priority class should be used for xyz pods only"


1. scheduling queue [ PrioritySort] 
2. filtering [NodeResourcesFit][NodeName][NodeUnschedulable]
3. scoring [NodeResourcesFit][ImageLocality]
4. binding [DefaultBinder]

* Extension Points [QueueSort, PreFilter, Filter, PostFilter,PreScore,score,reserve,permit,preBind,bind,postBind]

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler
   plugins:
     score:
        disabled:
         - name: TaintToleration
        enabled:
         - name: MyCustomPluginA
         - name: MyCustomPluginB


apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler-2
   plugins:
    preScore:
      disabled:
       - name: '*'
    score:
      enabled:
       - name: '*' 


https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md



https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/



https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/



https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

-------------------------------------

kubectl logs -f pod_name

------------------------

Application life cycle

kubectl create -f deployment-def.yaml
kubectl get deployments
kubeclt apply -f deployment-def.yaml
kubectl set image deployment/myapp-deployment nginx:1.9.1
kubectl rollout status deployment/myapp-deployment
kubectl history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

------

commands and arguments


apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: debian
    command: ["printenv"]
    args: ["HOSTNAME", "KUBERNETES_PORT"]
  restartPolicy: OnFailure

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
    args:
      - "1200"


---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"



Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]

-------------------------------------------

Configuring ENV variables in applications

  env: 
   
    - name: APP_COLOR
      value: pink
   
  env: 
   
    - name: APP_COLOR
      valueFrom:
          configMapKeyRef:
  
  env: 
   
    - name: APP_COLOR
      valueFrom:
          secretKeyRef:

----------------------------------


Create config maps

1.create cm
2. inject

imperative 

kubectl create configmap config-name --from-literal=key=value
kubectl create configmap app-config --from-literal=APP_COLOR=blue
or
kubectl create configmap app-config --from-file=app_config.properties

declartive

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR=blue
  APP_MODE= prod

kubectl create -f config-map.yaml

kubectl get configmaps
kubectl describe configmaps


pod injection 

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    envFrom:
      - configMapRef:
	     name: app-config
     or
   
  
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    env:
      - name: APP_COLOR
         configMapKeyRef:
           name: app-config
           key: APP_COLOR


or
   volumes:
   - name: app-config-volume
     configMap:
       name: app-config


---------------

create secrets then inject into pods

1.imperative.

kubectl create secret generic <secret-name> --from-literal=key=value
kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASSWD=passwd

kubectl create secret generic app-secret --from-file=app_secret.properties.

2. declarative


apiVersion: v1
kind: Secret
metadata:  
  name: app-secret
data:
  DB_HOST= mysql
  DB_USER= root
  DB_PASS= passwd 

encode strings in secret

echo -n 'mysql' | base64
echo -n 'root'  | base64
echo -n 'passwd' | base64

kubectl get secrets
kubectl describe secret
kubectl get app-secret -o YAML


kubectl create -f secret-data.yaml 

-----------------------
 Secrets in Pods

ENV


apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    envFrom:
      secretRef:
        name: app-secret


SINGLE ENV
   
    env:
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: app-secret  
	    key: DB_Password


Secrets in Pods as Volumes

     volumes:
     - name: app-secret-volume
       secret:
	 secretName: app-secret


Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 



Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here



Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.

kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
-----------------

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
--------------

Encryption at rest


kubectl create secret generic my-secret --from-literal=key1=supersecret
k describe secret my-secret
k get secret my-secret -o YAML

apiVersion: v1
data:
  key1: c3VwZXJzZWNyZXQ=
kind: Secret
metadata:
  creationTimestamp: "2022-10-27T23:24:49Z"
  name: my-secret
  namespace: default
  resourceVersion: "4478"
  uid: fae657ec-9cf0-493d-85bb-6870075a6d0e
type: Opaque


echo "c3VwZXJzZWNyZXQ=" | base64 --decode  ---> supersecret


if etcdctl is not there --> apt-get install etcd-client


kubectl get pod -n kube-system --> etcd-controlplane 

ls -ltr /etc/kubernetes/pki/etcd/ca.crt
ls -ltr /etc/kubernetes/pki/etcd/

ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/my-secret | hexdump -C


-->

00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 00  0a 0c 0a 02 76 31 12 06  |ret.k8s.....v1..|
00000030  53 65 63 72 65 74 12 d0  01 0a b0 01 0a 09 6d 79  |Secret........my|
00000040  2d 73 65 63 72 65 74 12  00 1a 07 64 65 66 61 75  |-secret....defau|
00000050  6c 74 22 00 2a 24 66 61  65 36 35 37 65 63 2d 39  |lt".*$fae657ec-9|
00000060  63 66 30 2d 34 39 33 64  2d 38 35 62 62 2d 36 38  |cf0-493d-85bb-68|
00000070  37 30 30 37 35 61 36 64  30 65 32 00 38 00 42 08  |70075a6d0e2.8.B.|
00000080  08 c1 a6 ec 9a 06 10 00  8a 01 61 0a 0e 6b 75 62  |..........a..kub|
00000090  65 63 74 6c 2d 63 72 65  61 74 65 12 06 55 70 64  |ectl-create..Upd|
000000a0  61 74 65 1a 02 76 31 22  08 08 c1 a6 ec 9a 06 10  |ate..v1"........|
000000b0  00 32 08 46 69 65 6c 64  73 56 31 3a 2d 0a 2b 7b  |.2.FieldsV1:-.+{|
000000c0  22 66 3a 64 61 74 61 22  3a 7b 22 2e 22 3a 7b 7d  |"f:data":{".":{}|
000000d0  2c 22 66 3a 6b 65 79 31  22 3a 7b 7d 7d 2c 22 66  |,"f:key1":{}},"f|
000000e0  3a 74 79 70 65 22 3a 7b  7d 7d 42 00 12 13 0a 04  |:type":{}}B.....|
000000f0  6b 65 79 31 12 0b 73 75  70 65 72 73 65 63 72 65  |key1..supersecre|   -->> secret not encrypted at rest
00000100  74 1a 06 4f 70 61 71 75  65 1a 00 22 00 0a        |t..Opaque.."..|
0000010e

ensure that encryption is enabled ( its not yet)

ps -aux | grep kube-api | grep encryption-provider-config

cat /etc/kubernetes/manifests/kube-api.yaml --> not encryption provider in contanier commands 

create new encryption config file

create random key -->  head -c 32 /dev/urandom | base64  --> Jwg11KNvQ3tjQR+NyevAtoHipbKtve7oymbnJfb1jVA=

vi enc.yml

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: Jwg11KNvQ3tjQR+NyevAtoHipbKtve7oymbnJfb1jVA=
      - identity: {}


add this line to kube-api -->  --encryption-provider-config=/etc/kubernetes/enc/enc.yaml
also add below volume mount

- name: enc                          
  mountPath: /etc/kubernetes/enc      
  readonly: true 

also add below volume

- name: enc                             
  hostPath:                             
  path: /etc/kubernetes/enc           
  type: DirectoryOrCreate    


save file then get pods or crictl pods

now its using encryption

controlplane $ ps -aux | grep kube-api | grep encryption
root       82736  5.6 16.9 1111876 345680 ?      Ssl  00:15   0:05 kube-apiserver --advertise-address=172.30.1.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --encryption-provider-config=/etc/kubernetes/enc/enc.yaml

mkdir /etc/kubernetes/enc 
mv enc.yaml /etc/kubernetes/enc 


create another secret 

kubectl create secret generic my-secret-2 --from-literal=key2=supersecret

ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/my-secret-2 | hexdump -C

encryption enabled -->


00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 2d 32 0a 6b 38  73 3a 65 6e 63 3a 61 65  |ret-2.k8s:enc:ae|
00000030  73 63 62 63 3a 76 31 3a  6b 65 79 31 3a c8 b7 e5  |scbc:v1:key1:...|
00000040  b0 5f b2 a9 52 77 2e 38  23 a0 d7 d5 43 32 10 fe  |._..Rw.8#...C2..|
00000050  42 3a 19 d0 b5 a6 b4 1c  46 3e 47 ad 4e 6b ea fc  |B:......F>G.Nk..|
00000060  ed aa d5 a7 57 85 d0 94  56 83 f6 5b 16 30 b0 13  |....W...V..[.0..|
00000070  46 04 f7 25 d1 68 4c 09  67 6a 3d 10 86 c6 e4 9c  |F..%.hL.gj=.....|
00000080  da ef 0d 19 a5 d9 e4 6b  ce 0b 1c ce 7f 58 6c 25  |.......k.....Xl%|
00000090  c3 ac bd fe 08 a9 d0 3d  70 3b e0 32 a3 ae 76 13  |.......=p;.2..v.|
000000a0  0f 5c af 25 0d 61 43 a9  f4 b5 f3 38 ce cf 10 5b  |.\.%.aC....8...[|
000000b0  af 78 1b fd f9 fd 9c 66  50 b4 1a 40 ce bc 68 cc  |.x.....fP..@..h.|
000000c0  1a 48 04 4f ca 70 d6 6f  c2 75 4e 5e 39 d6 47 9c  |.H.O.p.o.uN^9.G.|
000000d0  d8 64 3e 46 c1 37 21 c4  3b be f3 51 84 f1 1a 58  |.d>F.7!.;..Q...X|
000000e0  93 e7 9d 33 30 ef 9c 15  a4 08 e0 c2 f6 5e 5d 57  |...30........^]W|
000000f0  fe a4 5f aa a8 bc 9f 4b  31 38 f9 47 53 a6 c9 66  |.._....K18.GS..f|
00000100  22 dc 2f f1 80 2b a0 fc  65 89 0e 0d c3 77 ef 68  |"./..+..e....w.h|
00000110  32 68 17 33 8c 52 6f 0e  14 6b 9a 4b d6 98 0a f2  |2h.3.Ro..k.K....|
00000120  84 27 0e 92 7d d1 90 62  b7 d3 ff 77 cc 46 fc 4f  |.'..}..b...w.F.O|
00000130  94 8c 4d 19 8b 4e e0 05  9d 31 3c 16 44 0a        |..M..N...1<.D.|
0000013e

old one still not encrypted -->

0000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 00  0a 0c 0a 02 76 31 12 06  |ret.k8s.....v1..|
00000030  53 65 63 72 65 74 12 d0  01 0a b0 01 0a 09 6d 79  |Secret........my|
00000040  2d 73 65 63 72 65 74 12  00 1a 07 64 65 66 61 75  |-secret....defau|
00000050  6c 74 22 00 2a 24 66 61  65 36 35 37 65 63 2d 39  |lt".*$fae657ec-9|
00000060  63 66 30 2d 34 39 33 64  2d 38 35 62 62 2d 36 38  |cf0-493d-85bb-68|
00000070  37 30 30 37 35 61 36 64  30 65 32 00 38 00 42 08  |70075a6d0e2.8.B.|
00000080  08 c1 a6 ec 9a 06 10 00  8a 01 61 0a 0e 6b 75 62  |..........a..kub|
00000090  65 63 74 6c 2d 63 72 65  61 74 65 12 06 55 70 64  |ectl-create..Upd|
000000a0  61 74 65 1a 02 76 31 22  08 08 c1 a6 ec 9a 06 10  |ate..v1"........|
000000b0  00 32 08 46 69 65 6c 64  73 56 31 3a 2d 0a 2b 7b  |.2.FieldsV1:-.+{|
000000c0  22 66 3a 64 61 74 61 22  3a 7b 22 2e 22 3a 7b 7d  |"f:data":{".":{}|
000000d0  2c 22 66 3a 6b 65 79 31  22 3a 7b 7d 7d 2c 22 66  |,"f:key1":{}},"f|
000000e0  3a 74 79 70 65 22 3a 7b  7d 7d 42 00 12 13 0a 04  |:type":{}}B.....|
000000f0  6b 65 79 31 12 0b 73 75  70 65 72 73 65 63 72 65  |key1..supersecre|
00000100  74 1a 06 4f 70 61 71 75  65 1a 00 22 00 0a        |t..Opaque.."..|
0000010e


but

Since Secrets are encrypted on write, performing an update on a Secret will encrypt that content.

kubectl get secrets --all-namespaces -o json | kubectl replace -f -

secret/my-secret replaced
secret/my-secret-2 replaced
secret/bootstrap-token-metz81 replaced

now both are encrypted

00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 3a  65 6e 63 3a 61 65 73 63  |ret.k8s:enc:aesc|
00000030  62 63 3a 76 31 3a 6b 65  79 31 3a 3c 1d d7 b1 54  |bc:v1:key1:<...T|
00000040  11 82 09 de 1a 5b e5 e1  4c 40 8b ce 95 d3 45 d9  |.....[..L@....E.|
00000050  40 68 26 b8 d2 ca ae 6d  33 20 15 e6 4d 80 d1 94  |@h&....m3 ..M...|
00000060  30 d8 b7 3b 05 39 68 cb  88 a4 01 34 fa 88 b3 77  |0..;.9h....4...w|
00000070  08 5a a2 5f eb 5b 45 03  49 56 14 82 d3 36 9d d7  |.Z._.[E.IV...6..|
00000080  ea 41 62 7c 1a 1b c0 13  0f 9d 64 89 93 67 f2 0f  |.Ab|......d..g..|
00000090  57 35 b5 70 fb af 9a 90  8e ff 72 35 93 77 68 3d  |W5.p......r5.wh=|
000000a0  61 de e5 ad c6 02 1b 20  ca 41 e2 19 fb 41 60 f9  |a...... .A...A`.|
000000b0  70 45 80 9e be 11 f1 0a  8e a1 8e 5c ed b6 dd 54  |pE.........\...T|
000000c0  58 09 d2 ac 51 1b 77 f9  a6 3f 16 c6 85 0f f0 c1  |X...Q.w..?......|
000000d0  b1 e0 78 db ad 2d ed f3  d1 ae 9d ac 8f c6 5a 39  |..x..-........Z9|
000000e0  bf 55 f6 5d a9 9a 9e ef  08 d9 05 0c e6 d1 01 4b  |.U.]...........K|
000000f0  1a 7a 2a e4 ee 70 96 95  b7 74 06 be b1 d2 4e 14  |.z*..p...t....N.|
00000100  53 9a e2 2c 7c a8 ec 8c  5f 70 2d 1d 62 65 0f 7f  |S..,|..._p-.be..|
00000110  e7 fc 66 87 74 e8 41 6f  71 09 73 b2 07 ee be 53  |..f.t.Aoq.s....S|
00000120  5b 52 1e f7 9a e2 43 71  aa 4a 01 fa 16 0e 64 22  |[R....Cq.J....d"|
00000130  c8 ec 75 92 d1 8a 7e 7d  09 c5 30 0a              |..u...~}..0.|
0000013c

----------------

Multi containers pods


Init Containers

InitContainers
In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.



But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts. That's where initContainers comes in.



An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section,  like this:



apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']


When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. 

You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order.

If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']


Read more about initContainers here. And try out the upcoming practice test.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/


spec:

  initContainers:
  - name: init-red 
    image: busybox
    command:
      - "sleep"
      - "20"  
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28

-------------

OS upgrades

pod eviction time out --> default 5 mins

kubectl drain node-1 ---> pods are shutdown gracefully and moved to another node
kubectl cordon node-1 ---> marks it unscheduable 

when upgrade is done and its online its still not available
u have to kubectl uncordon node-1

k drain node01 --ignore-daemonsets 
k uncordon node01 ---> only new pods will be scheduled on node01
------

K8s software releases

v1.11.3   
1. major  11. minor ( features & functionality)  3.patch (bug fixes)

etcd cluster has its own release, also coreDNS

K8s components

1. kube-api-server ( has to be highest version others has to be below or same)
2. controller-manager
3. kube-scheudler
4. kubelet
5.kube-proxy
6.kubectl
7.etcd cluster
8.coreDNS


upgrade master first, workers stay up.
or
all workers upgraded, no user access no apps
or
one worker upgraded one by one.


kubeadm upgrade plan ( we can go one minor version at a time)

apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply v.12.0

apt-upgrade -y kubectl=1.12.0-00
systemctl restart kubelet
kubectl get nodes -- see version 

kubectl drain node-1
apt-get upgrade -y kubeadm=1.12.0-00
apt-upgrade -y kubectl=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.12.0
kubectl uncordon node-1
-----

UPGRADE

check kubeadm latest version

 kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.24.0
[upgrade/versions] kubeadm version: v1.23.0
I1030 01:25:39.842509    7408 version.go:255] remote version is much newer: v1.25.3; falling back to: stable-1.23
[upgrade/versions] Target version: v1.23.13

** upgrading control plane first

controlplane ~ ✖ kubectl drain controlplane --ignore-daemonsets 
node/controlplane already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-f7kjb, kube-system/kube-proxy-xzwpb
evicting pod kube-system/coredns-6d4b75cb6d-rd58f
evicting pod default/blue-797fc567b4-jtspm
evicting pod kube-system/coredns-6d4b75cb6d-k4gdk
evicting pod default/blue-797fc567b4-cqb5t


Upgrade the controlplane components to exact version v1.24.0

Upgrade the kubeadm tool (if not already), then the controlplane components, and finally the kubelet. Practice referring to the Kubernetes documentation page.
Note: While upgrading kubelet, if you hit dependency issues while running the apt-get upgrade kubelet command, use the apt install kubelet=1.24.0-00 command instead.


apt-update
apt-cache show kubeadm | grep ^Version
apt-get install kubeadm=1.24.0-00

kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.0", GitCommit:"4ce5a8954017644c5420bae81d72b09b735c21f0", GitTreeState:"clean", BuildDate:"2022-05-03T13:44:24Z", GoVersion:"go1.18.1", Compiler:"gc", Platform:"linux/amd64"}



-------------

apt-cache show kubectl | grep ^Version
apt-get install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet

controlplane ~ ➜  kubectl version
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.0", GitCommit:"ab69524f795c42094a6630298ff53f3c3ebab7f4", GitTreeState:"clean", BuildDate:"2021-12-07T18:16:20Z", GoVersion:"go1.17.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.0", GitCommit:"4ce5a8954017644c5420bae81d72b09b735c21f0", GitTreeState:"clean", BuildDate:"2022-05-03T13:38:19Z", GoVersion:"go1.18.1", Compiler:"gc", Platform:"linux/amd64"}

kubectl uncordon controlplane

---------------

upgrade worker node

kubectl drain node01 --ignore-daemonsets
ssh to the node 
ssh node01
apt-get update
apt-get install kubeadm=1.24.0-00
kubeadm upgrade node


apt-get install kubelet=1.24.0-00
systemctl daemon-reload
systemctl restart kubelet
kubectl uncordon node01

-------------------

backup and restore

candidates:

1. resource configuration
2. etcd cluster
3. Persistent volumes

Its better to backup resource configs from kube-api server

kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml 

etcd cluster --> state of kluster
etcd.service --> --data-dir=/var/lib/etcd

backup ETCD

ETCDCTL_API=3 etcdctl snapshot save snapshot.db
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt
--cert=/etc/etcd/etcd-server.crt
--key=/etc/etcd

ETCDCTL_API=3 etcdctl snapshot status snapshot.db
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt
--cert=/etc/etcd/etcd-server.crt
--key=/etc/etcd

restore ETCD

service kube-apiserver stop

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db 
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt
--cert=/etc/etcd/etcd-server.crt
--key=/etc/etcd
--data-dir /var/lib/etcd-from-backup


etcdctl is a command line client for etcd.



In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.



You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

export ETCDCTL_API=3

On the Master Node:

export ETCDCTL_API=3
etcdctl version
etcdctl version 3.3.13
API version 3.3



To see all the options for a specific sub-command, make use of the -h or --help flag.



For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.



Since our ETCD database is TLS-Enabled, the following options are mandatory:

--cacert  verify certificates of TLS-enabled secure servers using this CA bundle

--cert    identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]  This is the default as ETCD is running on master node and exposed on localhost 2379.

--key     identify secure client using this TLS key file





Similarly use the help option for snapshot restore to see all available options for restoring the backup.

etcdctl snapshot restore -h

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, check out the solution video for the Backup and Restore Lab.



configure new dir in etcd.service  --data-dir=/var/lib/etcd-from-backup


systemctl etcd restart

service kube-apiserver start

for ETCDCTL_API=3 command specify these as well
--endpoints=https://127.0.0.1:2379
--cacert=/etc/etcd/ca.crt
--cert=/etc/etcd/etcd-server.crt
--key=/etc/etcd


----------------

Get current etcd version

1.  kubectl describe pod etcd-controlplane -n kube-system | grep Image
    Image:         k8s.gcr.io/etcd:3.5.3-0

2. kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'

etcd-version":"3.5.3"


At what address can you reach the ETCD cluster from the controlplane node?


Check the ETCD Service configuration in the ETCD POD

kubectl -n kube-system get pod  etcd-controlplane -o YAML | grep listen-client-urls 

 - --listen-client-urls=https://127.0.0.1:2379,https://10.2.255.3:2379


/etc/kubernetes/pki/etcd/server.crt ---> server certificate file

/etc/kubernetes/pki/etcd/ca.crt --> ca cert file


The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.

Store the backup file at location /opt/snapshot-pre-boot.db


ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> snapshot save /opt/snapshot-pre-boot.db
Snapshot saved at /opt/snapshot-pre-boot.db


services, pods, deployments gone
restore etcd

Luckily we took a backup. Restore the original state of the cluster using the backup file.

ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db

update etcd.yaml in manifest with new etcd backup path

- hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate



root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2022-03-25 09:19:27.175043 I | mvcc: restore compact to 2552
2022-03-25 09:19:27.266709 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
root@controlplane:~# 


Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.



Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

 volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data


With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want).

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.



Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.



If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)



note: volume mounts refer to the dir mounted at container level
      volumes refer to dir in host (host_path) kub

---------------

https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

https://www.youtube.com/watch?v=qRPNuT080Hk

-----

 kubectl config
Modify kubeconfig files using subcommands like "kubectl config set current-context my-context"

 The loading order follows these rules:

  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes
place.
  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for
your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When
a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the
last file in the list.
  3.  Otherwise, ${HOME}/.kube/config is used and no merging takes place.

Available Commands:
  current-context Display the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  delete-user     Delete the specified user from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  get-users       Display users defined in the kubeconfig
  rename-context  Rename a context from the kubeconfig file
  set             Set an individual value in a kubeconfig file
  set-cluster     Set a cluster entry in kubeconfig
  set-context     Set a context entry in kubeconfig
  set-credentials Set a user entry in kubeconfig
  unset           Unset an individual value in a kubeconfig file
  use-context     Set the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file

Usage:
  kubectl config SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).





student-node ~ ➜  kubectl config get-contexts 
CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
*         cluster1   cluster1   cluster1   
          cluster2   cluster2   cluster2   

student-node ~ ➜  kubectl config use-context cluster1 
Switched to context "cluster1".

You can SSH to all the nodes (of both clusters) from the student-node.

For example:

student-node ~ ➜  ssh cluster1-controlplane
Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-1086-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.

cluster1-controlplane ~ ➜ 
To get back to the student node, use the logout or exit command, or, hit Control +D

cluster1-controlplane ~ ➜  logout
Connection to cluster1-controlplane closed.

student-node ~ ➜  


external etcd config ---> kube-api-server.yaml

in etcd-server node which has local etcd configured

ETCDCTL_API=3 etcdctl \
 --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/etcd/pki/ca.pem \
 --cert=/etc/etcd/pki/etcd.pem \
 --key=/etc/etcd/pki/etcd-key.pem \
  member list


Take a backup of etcd on cluster1 and save it on the student-node at the path /opt/cluster1.db


If needed, make sure to set the context to cluster1 (on the student-node):

student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  

student-node /etc ✖ kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.1.36.24:2379
      --advertise-client-urls=https://10.1.36.24:2379

student-node /etc ➜  kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep pki                  
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
    Path:          /etc/kubernetes/pki/etcd


SSH to the controlplane node of cluster1 and then take the backup using the endpoints and certificates we identified above:

ETCD backup is taken from cluster control plane NODE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

student-node /etc ✖ kubectl get nodes
NAME                    STATUS   ROLES           AGE   VERSION
cluster1-controlplane   Ready    control-plane   70m   v1.24.0
cluster1-node01         Ready    <none>          69m   v1.24.0

student-node /etc ➜  ssh cluster1-controlplane
Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-1092-gcp x86_64)


ETCDCTL_API=3 etcdctl --endpoints=https://10.1.220.8:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=snapshot save /opt/cluster1.db

Finally, copy the backup to the student-node. To do this, go back to the student-node and use scp as shown below:

student-node ~ ➜  scp cluster1-controlplane:/opt/cluster1.db /opt
cluster1.db    

why i have error context expired

https://medium.com/@shoaibjdev/etcdctl-error-context-deadline-exceeded-accessing-etcd-control-in-newer-versions-of-kubernetes-ef78c83be6ce     

--------------------------------------------------------------------

SECURITY!!!!!

1. Password based authenticaion disabled
2. SSH key based authentication 

Authentication

Admins
Developers
End Users
Bots -- service accounts

you cant create users kubectl create user 
but we can create service accounts
---> kubectl create serviceaccount sa1
---> kubectl get serviceaccount 

Users managed by kube-api server

Auth mechanisim  - basic 
Static Password File  -- users-details.csv -- passwd,userID,UID
kube-apiserver.service --> 
/etc/kubernetes/manifests/kube-apiserver.yaml  --basic-auth-file=users-details.csv 

authenticate user 
curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123" 

we add group to static pass file
passwd,user,uid,group1 

Static Token File

user-token-details.csv 

--token-auth-file=user-details.csv



Certificates
Identity Services 

------------

TLS transport layer sec

1. symmetric encryption  -- same key in client and server ( not safe)
2. asymmetric encryption -- private key, public lock  --> ssh-keygen --> id_rsa --> id_rsa.pub
certificate authority, CS-request 
PKI - Public Key Infrastructure

Certificate ( Public Key) *.crt, *.pem 
server.crt
server.pem
client.crt
client.pem

Private Key - *.key *-key.pem

server.key
server-key.pem
client.key
client-key.pem

root certificates --> certificate authority  --> ca.cert / ca.key

server certificates 

master node 
kube-api-server --> apiserver.crt(pub)  apiserver.key(prv)
etcd server --> etcdserver.crt     etcdserver.key

worker-node
kubelet server --> kubelet.crt --> kubelet.key 

client certificates

admin to access kube-api with kubectl REST API  --> admin.crt  admin.key 
kube-scheduler to access kube-api server --> scheduler.crt  scheduler.key 
kube-controller-manager to access kube-api server -->  controller-manager.crt  controller-manager.key
kube-proxy to access kube-api server --> kube-proxy.crt  kube-proxy.key 

####
kube-api server to connect to etcd server -- can use existing or api-server-etcdclient.crt / apiserver-etcd-client.key 
kube-api server to connect to kubelet server on worker nodes -- either use existing or api-server-client.crt / api-server-etcd-client.key

--- certificate creation 

EASYRSA / OPENSSL / CFSSL 

generate keys openssl genrsa -out ca.key 2048 --> ca.key 
certificate sign request --> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr -- > ca.csr
sign certificates --> openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt -- > ca.crt

--------

client side

ADMIN USER

generate keys openssl genrsa -out admin.key 2048 --> admin.key
certificate sign request --> openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr -- > admin.csr
sign certificate --> openssl x509 -req in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt --> admin.crt

curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt

kube-config.yaml 

apiVersion: v1
clusters:
- cluster: 
    certificate-authority: ca.crt
    server: https://kube-apiserver:6443
  name: kubernetes
kind: Config
users:
- name: kubernetes-admin
  user:
    client-certificate: admin.crt
    client-key: admin.key

-------
server side

ETCD servers

etcd can be deployed as a cluster on HA we must generate additional peer certificates
etcdserver.crt etcdserver.key
etcdpeer1.crt  etcdpeer1.key
etcdpeer2.crt  etcdpeer2.key

etcd.yaml

- --key-file=/path/etcdserver.key
- --cert-file=/path/etcdserver.crt

- --peer-cert-file=/path/etcdpeer1.crt
- --peer-client-cert-auth=true
- --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
- --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
- --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt

kube-api-server

3 sets of certs -- one for master node, one for etcd , one for kubelet 

generate key -- >openssl genrsa -out apiserver.key 2048 
csr --> openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.conf

to specify alternate names we create openssl config file 

openssl.conf --> pass this file as argument in CSR 
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetees.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.3l.23.12
IP.2 = 172.17.0.87

sign cert --> openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt --> apiserver.crt


PASS THESE KEYS/CERTS/CLIENT CERTS to kube-api server exec

ExecStart= /usr/local/bin/kube-apiserver
--etcd-cafile=/var/lib/kubernetes/ca.pem
--etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt
--etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key

--client-ca-file=/var/lib/kubernetes/ca.pem
--tls-cert-file=/var/lib/kubernetes/apiserver.crt
--tls-private-key-file=/var/lib/kubernetes/apiserver.key

--kubelet-certificate-authority=/var/lib/kubernetes/ca.pem
--kubelet-client-certificate=/var/lib/kubernetes/api-server-kubelet-client.crt
--kubelet-client-key=/var/lib/kubernetes/apiserver-kubelet-client.key

--------------------------

kubernetes certificate health checker --- excel sheet

1.certificate authority 
--> server --> cert /etc/kubernetes/pki/ca.crt --> CN_Name = kubernetes -> CA server root certificates for Kubernetes API Server
               key /etc/kubernetes/pki/ca.key  --> CA server root certificate key for Kubernetes API Server


openssl x509 -in /etc/kubernetes/pki/ca.crt -text -noout
check issuer, check subject, check expire date, check subject alternate name 

-------------------
2. kube-apiserver 
server --> cert -->  /etc/kubernetes/pki/apiserver.crt --> CN_NAME = kube-apiserver --> Certificate to serve Kube-api server
           key   --> /etc/kubernetes/pki/apiserver.key --> Key to serve Kube-api server

openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

kube-api server->  /etc/kubernetes/pki/ca.crt -> CN_Name = kubernetes -> CA Certificate to validate clients connecting to Kube-API Server
kubelet client -> /etc/kubernetes/pki/apiserver-kubelet-client.crt --> system:masters, CN = kube-apiserver-kubelet-client -> Client Certificate for Kube-API Server to connect to Kubelet
                  /etc/kubernetes/pki/apiserver-kubelet-client.key  -> Client Key for Kube-API Server to connect to Kubelet

openssl x509 -in /etc/kubernetes/pki/apiserver-kubelet-client.crt-text -noout

kubelet client -> /etc/kubernetes/pki/apiserver-etcd-client.crt -> system:masters, CN = kube-apiserver-etcd-client -> Client Certificate for Kube-API Server to connect to ETCD Server
                  /etc/kubernetes/pki/apiserver-etcd-client.key -> Client Key for Kube-API Server to connect to ETCD Server


Client CA File: Kube API Server to ETCD   --> /etc/kubernetes/pki/etcd/ca.crt--> Subject: CN = etcd-ca --> "CA File to validate Kube-API server to ETCD Server Connectivity.
 The ETCD setup can have a separate CA"
------------------

3. kubelet

server -->  /var/lib/kubelet/pki/kubelet.crt
            /var/lib/kubelet/pki/kubelet.key

            /var/lib/kubelet/pki/kubelet-client-2022-10-18-15-28-17.pem


------------------------------
4. certificate authority (etcd)

server -->  /etc/kubernetes/pki/etcd/ca.crt  --> 

CA Server root certificates for ETCD Server. 
(This could be the same as kube-api server or a separate one of its own.)

/etc/kubernetes/pki/etcd/ca.key -->

CA Server root certificate key for ETCD Server.
 (This could be the same as kube-api server or a separate one of its own.)

---------------
5. etcd server

/etc/kubernetes/pki/etcd/server.crt -->  Certificates for ETCD Server

/etc/kubernetes/pki/etcd/server.key --> ETCD Server Certificate Key

Checks to perform:
1. Make sure the correct CN and ALT names, Organization are present. Specifically for the kube-api server and the nodes(kubelets).
2. Ensure the certificates are not expired.
3. Ensure the certificates are issued by the right CA.
4. Ensure the correct certificate path is provided in the options on the service configuration files

-------------------------------------------------------------------------------------------------------

crictl ps -a

crictl logs container_id

-----

certificate API 

1. create certificate sign request object
2. review request
3. approve request 
4. share certs to users

openssl genrsa -out jane.key 2048 --> jane.key
openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr--> jane.csr

jane-csr.yaml 

kubectl get csr
kubectl certificate approve jane

controller-manager does all csr approve - csr signing 

kube-controller-manager.yaml 

- --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
- --cluster-signing-key-file=/etc/kubernetes/pki/ca.key 

----

controlplane ~ ➜  cat akshay.csr
-----BEGIN CERTIFICATE REQUEST-----
MIICVjCCAT4CAQAwETEPMA0GA1UEAwwGYWtzaGF5MIIBIjANBgkqhkiG9w0BAQEF
AAOCAQ8AMIIBCgKCAQEAxh4HeSrlTuRYvgvmkf4NuqVPZoFehKXPotGkG30pMktC
vxWkQ+axQG82oMF38QQNxLdVkbOemI1d4HVtw1prka0za4HFtjZr0c9scvTgRZJt
zIrHHWO4viMOzsg+MczEgHuI3ES16Tn1Is7yIU9wAjUfRQdeBM1XjVa0OLfORUH8
54nBU8oDq1eIszegeoLzZiFd2kbgdJRhMlZEeZa0BN+ZoMLddXfcJ458t7Bxqjsr
LyG8Qtb38AocskUWyCyinYzI1GLOppEAlkLt3xZEpX9OBkCx/ereZcrnAhCjQLBR
iZSae6AhrwLX2vPpoN/ojp1qYwnU2OfXuTDQECyrpQIDAQABoAAwDQYJKoZIhvcN
AQELBQADggEBAGiatyMh7VRHSaW12mEgXLh3OQwzyl1qJr+vn7NAj3RdRD3ra03G
++NlSshUtlav32LwFQCYKaPTMuIoLQdvn0pALo0Hp0Ssu/FCVdcXXVEMhtj4cXgR
JSkmSuO5Gg5Tk03vwCDabtO+01/oiLCPWx07gaLSYt2Ks8FE18sRLdfFmjt2Prsm
GXHr6iVQlmQnn0y0r1MGUgpKntUkrxXmNC3fG+w9Jobnz1Xdxa6BUsTQiaYpcKQk
OAu9KkTxQkB4Bc6nVw3u6gR3iHwUK4cGjA/LSZdoVQ3ChIk77Hmrg2/NsdmF5Na7
Q22vdskBPqYOwuFwdRVX/TX+BdCfWSeZXV0=
-----END CERTIFICATE REQUEST-----

Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file

As of kubernetes 1.19, the API to use for CSR is certificates.k8s.io/v1.

Please note that an additional field called signerName should also be added when creating CSR. For client authentication to the API server we will use the built-in signer kubernetes.io/kube-apiserver-client.

CheckCompleteIncomplete


	
cat akshay.csr | base64 | tr -d "\n"
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXhoNEhlU3JsVHVSWXZndm1rZjROdXFWUFpvRmVoS1hQb3RHa0czMHBNa3RDCnZ4V2tRK2F4UUc4Mm9NRjM4UVFOeExkVmtiT2VtSTFkNEhWdHcxcHJrYTB6YTRIRnRqWnIwYzlzY3ZUZ1JaSnQKeklySEhXTzR2aU1PenNnK01jekVnSHVJM0VTMTZUbjFJczd5SVU5d0FqVWZSUWRlQk0xWGpWYTBPTGZPUlVIOAo1NG5CVThvRHExZUlzemVnZW9MelppRmQya2JnZEpSaE1sWkVlWmEwQk4rWm9NTGRkWGZjSjQ1OHQ3QnhxanNyCkx5RzhRdGIzOEFvY3NrVVd5Q3lpbll6STFHTE9wcEVBbGtMdDN4WkVwWDlPQmtDeC9lcmVaY3JuQWhDalFMQlIKaVpTYWU2QWhyd0xYMnZQcG9OL29qcDFxWXduVTJPZlh1VERRRUN5cnBRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR2lhdHlNaDdWUkhTYVcxMm1FZ1hMaDNPUXd6eWwxcUpyK3ZuN05BajNSZFJEM3JhMDNHCisrTmxTc2hVdGxhdjMyTHdGUUNZS2FQVE11SW9MUWR2bjBwQUxvMEhwMFNzdS9GQ1ZkY1hYVkVNaHRqNGNYZ1IKSlNrbVN1TzVHZzVUazAzdndDRGFidE8rMDEvb2lMQ1BXeDA3Z2FMU1l0MktzOEZFMThzUkxkZkZtanQyUHJzbQpHWEhyNmlWUWxtUW5uMHkwcjFNR1VncEtudFVrcnhYbU5DM2ZHK3c5Sm9ibnoxWGR4YTZCVXNUUWlhWXBjS1FrCk9BdTlLa1R4UWtCNEJjNm5WdzN1NmdSM2lId1VLNGNHakEvTFNaZG9WUTNDaElrNzdIbXJnMi9Oc2RtRjVOYTcKUTIydmRza0JQcVlPd3VGd2RSVlgvVFgrQmRDZldTZVpYVjA9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=

or

cat akshay.csr | base64 -w 0

akshray!

apiVersion: certificates.k8s.io/v1
kind: certificateSigningRequest
metadata:
  name: akshray
spec:
  groups:
  - system:authenticated
  usages:
  - client auth
  signerName: kubernetes.io/kube-apiserver-client
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXhoNEhlU3JsVHVSWXZndm1rZjROdXFWUFpvRmVoS1hQb3RHa0czMHBNa3RDCnZ4V2tRK2F4UUc4Mm9NRjM4UVFOeExkVmtiT2VtSTFkNEhWdHcxcHJrYTB6YTRIRnRqWnIwYzlzY3ZUZ1JaSnQKeklySEhXTzR2aU1PenNnK01jekVnSHVJM0VTMTZUbjFJczd5SVU5d0FqVWZSUWRlQk0xWGpWYTBPTGZPUlVIOAo1NG5CVThvRHExZUlzemVnZW9MelppRmQya2JnZEpSaE1sWkVlWmEwQk4rWm9NTGRkWGZjSjQ1OHQ3QnhxanNyCkx5RzhRdGIzOEFvY3NrVVd5Q3lpbll6STFHTE9wcEVBbGtMdDN4WkVwWDlPQmtDeC9lcmVaY3JuQWhDalFMQlIKaVpTYWU2QWhyd0xYMnZQcG9OL29qcDFxWXduVTJPZlh1VERRRUN5cnBRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR2lhdHlNaDdWUkhTYVcxMm1FZ1hMaDNPUXd6eWwxcUpyK3ZuN05BajNSZFJEM3JhMDNHCisrTmxTc2hVdGxhdjMyTHdGUUNZS2FQVE11SW9MUWR2bjBwQUxvMEhwMFNzdS9GQ1ZkY1hYVkVNaHRqNGNYZ1IKSlNrbVN1TzVHZzVUazAzdndDRGFidE8rMDEvb2lMQ1BXeDA3Z2FMU1l0MktzOEZFMThzUkxkZkZtanQyUHJzbQpHWEhyNmlWUWxtUW5uMHkwcjFNR1VncEtudFVrcnhYbU5DM2ZHK3c5Sm9ibnoxWGR4YTZCVXNUUWlhWXBjS1FrCk9BdTlLa1R4UWtCNEJjNm5WdzN1NmdSM2lId1VLNGNHakEvTFNaZG9WUTNDaElrNzdIbXJnMi9Oc2RtRjVOYTcKUTIydmRza0JQcVlPd3VGd2RSVlgvVFgrQmRDZldTZVpYVjA9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
   

no matches for kind "certificateSigningRequest" in version "certificates.k8s.io/v1"
ensure CRDs are installed first


--> found the problem! this is sick its should be : - system:authenticated and not - system: authenticated ( see the space)

cat <<EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZVzVuWld4aE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTByczhJTHRHdTYxakx2dHhWTTJSVlRWMDNHWlJTWWw0dWluVWo4RElaWjBOCnR2MUZtRVFSd3VoaUZsOFEzcWl0Qm0wMUFSMkNJVXBGd2ZzSjZ4MXF3ckJzVkhZbGlBNVhwRVpZM3ExcGswSDQKM3Z3aGJlK1o2MVNrVHF5SVBYUUwrTWM5T1Nsbm0xb0R2N0NtSkZNMUlMRVI3QTVGZnZKOEdFRjJ6dHBoaUlFMwpub1dtdHNZb3JuT2wzc2lHQ2ZGZzR4Zmd4eW8ybmlneFNVekl1bXNnVm9PM2ttT0x1RVF6cXpkakJ3TFJXbWlECklmMXBMWnoyalVnald4UkhCM1gyWnVVV1d1T09PZnpXM01LaE8ybHEvZi9DdS8wYk83c0x0MCt3U2ZMSU91TFcKcW90blZtRmxMMytqTy82WDNDKzBERHk5aUtwbXJjVDBnWGZLemE1dHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR05WdmVIOGR4ZzNvK21VeVRkbmFjVmQ1N24zSkExdnZEU1JWREkyQTZ1eXN3ZFp1L1BVCkkwZXpZWFV0RVNnSk1IRmQycVVNMjNuNVJsSXJ3R0xuUXFISUh5VStWWHhsdnZsRnpNOVpEWllSTmU3QlJvYXgKQVlEdUI5STZXT3FYbkFvczFqRmxNUG5NbFpqdU5kSGxpT1BjTU1oNndLaTZzZFhpVStHYTJ2RUVLY01jSVUyRgpvU2djUWdMYTk0aEpacGk3ZnNMdm1OQUxoT045UHdNMGM1dVJVejV4T0dGMUtCbWRSeEgvbUNOS2JKYjFRQm1HCkkwYitEUEdaTktXTU0xMzhIQXdoV0tkNjVoVHdYOWl4V3ZHMkh4TG1WQzg0L1BHT0tWQW9FNkpsYWFHdTlQVmkKdjlOSjVaZlZrcXdCd0hKbzZXdk9xVlA3SVFjZmg3d0drWm89Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
EOF


controlplane ~ ➜  k get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
akshay      40s   kubernetes.io/kube-apiserver-client           kubernetes-admin           24h                 Pending
csr-9hff7   41m   kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   <none>              App


controlplane ~ ➜  k certificate approve akshay
certificatesigningrequest.certificates.k8s.io/akshay approved


controlplane ~ ➜  k get csr agent-smith -o YAML 
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  creationTimestamp: "2022-11-09T02:09:18Z"
  name: agent-smith
  resourceVersion: "3621"
  uid: 5d725422-cfe3-4d22-8086-d3b93418481d
spec:
  groups:
  - system:masters
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1dEQ0NBVUFDQVFBd0V6RVJNQThHQTFVRUF3d0libVYzTFhWelpYSXdnZ0VpTUEwR0NTcUdTSWIzRFFFQgpBUVVBQTRJQkR3QXdnZ0VLQW9JQkFRRE8wV0pXK0RYc0FKU0lyanBObzV2UklCcGxuemcrNnhjOStVVndrS2kwCkxmQzI3dCsxZUVuT041TXVxOTlOZXZtTUVPbnJEVU8vdGh5VnFQMncyWE5JRFJYall5RjQwRmJtRCs1eld5Q0sKeTNCaWhoQjkzTUo3T3FsM1VUdlo4VEVMcXlhRGtuUmwvanYvU3hnWGtvazBBQlVUcFdNeDRCcFNpS2IwVSt0RQpJRjVueEF0dE1Wa0RQUTdOYmVaUkc0M2IrUVdsVkdSL3o2RFdPZkpuYmZlek90YUF5ZEdMVFpGQy93VHB6NTJrCkVjQ1hBd3FDaGpCTGt6MkJIUFI0Sjg5RDZYYjhrMzlwdTZqcHluZ1Y2dVAwdEliT3pwcU52MFkwcWRFWnB3bXcKajJxRUwraFpFV2trRno4MGxOTnR5VDVMeE1xRU5EQ25JZ3dDNEdaaVJHYnJBZ01CQUFHZ0FEQU5CZ2txaGtpRwo5dzBCQVFzRkFBT0NBUUVBUzlpUzZDMXV4VHVmNUJCWVNVN1FGUUhVemFsTnhBZFlzYU9SUlFOd0had0hxR2k0CmhPSzRhMnp5TnlpNDRPT2lqeWFENnRVVzhEU3hrcjhCTEs4S2czc3JSRXRKcWw1ckxaeTlMUlZyc0pnaEQ0Z1kKUDlOTCthRFJTeFJPVlNxQmFCMm5XZVlwTTVjSjVURjUzbGVzTlNOTUxRMisrUk1uakRRSjdqdVBFaWM4L2RoawpXcjJFVU02VWF3enlrcmRISW13VHYybWxNWTBSK0ROdFYxWWllKzBIOS9ZRWx0K0ZTR2poNUw1WVV2STFEcWl5CjRsM0UveTNxTDcxV2ZBY3VIM09zVnBVVW5RSVNNZFFzMHFXQ3NiRTU2Q0M1RGhQR1pJcFVibktVcEF3a2ErOEUKdndRMDdqRytocGtueG11RkFlWHhnVXdvZEFMYUo3anUvVERJY3c9PQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - server auth
  username: agent-x
status: {}


controlplane ~ ➜  k certificate deny agent-smith
certificatesigningrequest.certificates.k8s.io/agent-smith denied

controlplane ~ ➜  k delete csr agent-smith 
certificatesigningrequest.certificates.k8s.io "agent-smith" deleted
------------------

KUBECONFIG

kubectl get pods --server my-kube-playground:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt 

KubeConfig File

$HOME/.kube/config 

Clusters -- Development, Production, Google
Contexts -- Admin@Production, Dev@Google
Users    -- Admin, Dev User, Prod User


apiVersion: v1
kind: Config

clusters:
- name: my-kube-playground
  clusters:
    certificate-authority:
    server: https://my-kube-playground:6443

contexts:

- name: my-kube-admin@my-kube-playground
  context:
    cluster: my-kube-playground
    user:
          my-kube-admin

users:
- name: my-kube-admin
  user:
     client-certificate: admin.crt
     client-key: admin.key

-----------------------------------------



apiVersion: v1
kind: Config

current-context: dev-user@google

clusters:

- name: my-kube-playground
- name: development
- name: production
- name: google

contexts:
- name: my-kube-admin@my-kube-playground
- name: dev-user@google
- name: prod-user@production

users:
- name: my-kube-admin
- name: admin
- name: dev-user
- name: prod-user

----------------------

kubectl config view 

kubectl config use-context prod-user@production 

kubectl config -h

Certificates in KubeConfig:

cat ca.crt | base64


apiVersion: v1
kind: Config

clusters:
- name: Production
  cluster:
    certificate-authority= /etc/kubernetes/pki/ca.crt
    or
    certificate-authority-data= LS01ufuuuu


--------------------


controlplane ~ ➜  cat my-kube-config
apiVersion: v1
kind: Config

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}


I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that.

Once the right context is identified, use the kubectl config use-context command

-->  kubectl config --kubeconfig=/root/my-kube-config use-context research 
-->  kubectl config --kubeconfig=/root/my-kube-config current-context

We don't want to have to specify the kubeconfig file option on each command. Make the my-kube-config file the default kubeconfig.

copy and paste the config file into the default file $HOME/.kube/config


Persistent Key/Value Store
We have already seen how to secure the ETCD key/value store using TLS certificates in the previous videos. Towards the end of this course, when we setup an actual cluster from scratch we will see this in action.


-------------------------------------------

API GROUPS

curl https://kube-master:6443/version

--core
curl https://kube-master:6443/api/v1/pods-namespaces-events-endpoints-secrets-PV-PVC-services-configmaps-bindings-events-nodes-rc

/API - core
/metrics /healthz /version /api /apis  /logs


--named
/APIS


/apps  /extensions /networking.k8s.io /storage.k8s.io /authentication.k8s.io  /certificates.k8s.io 

/v1

resources
--> /deployments
--> /replicasets
--> /statefulsets  --> list, get, create,delete,update,watch



controlplane $ curl https://controlplane:6443 -k     
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}

you cant access directly without authentications

controlplane $ curl https://controlplane:6443 -k --key /etc/kubernetes/pki/ca.key --

curl https://controlplane:6443 -k --key admin.key --cert admin.crt --cacert ca.crt
or

kubectl proxy 
 
------------

RBAC 


-Developer 

can view PODS
can create PODS
can delete PODS
can create ConfigMaps

developer-role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer

rules:
- apiGroups: [""]
- resources: ["pods"]
- verbs: ["list","get","create","update","delete"]


devuser-developer-binding.yaml


apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io/v1
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io/v1

kubectl create -f devuser-developer-binding.yaml

kubectl get roles
kubectl describe role developer
kubectl get rolebindings 
kubectl describe rolebinding devuser-developer-binding

as user --> kubectl auth can-i create deployments
	    kubectl auth can-i delete nodes

   	    kubectl auth can-i create deployments --as dev-user
	    kubectl auth can-i create pods --as dev-user


apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer

rules:
- apiGroups: [""]
- resources: ["pods"]
- verbs: ["list","get","create","update","delete"]
- resourceNames: ["blue","orange"]

----

kube-apiserver:
    Container ID:  containerd://3bdc64b6cefc03bd43a5567eba3bb5ed110a508cbda90b7a170c0925c64314d5
    Image:         k8s.gcr.io/kube-apiserver:v1.24.0
    Image ID:      k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=10.61.178.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC


controlplane ~ ➜  kubectl describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 [kube-proxy]    [get]

--> kube-proxy role can get details of configmap object by the name kube-proxy only

Which account is the kube-proxy role assigned to?


controlplane ~ ➜  kubectl describe rolebindings kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  kube-proxy
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token  

controlplane ~ ➜  kubectl auth can-i get pods --as dev-user
no

Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.

Use the given spec:

CheckCompleteIncomplete
Role: developer
Role Resources: pods
Role Actions: list
Role Actions: create
Role Actions: delete
RoleBinding: dev-user-binding
RoleBinding: Bound to dev-user


kubectl create role developer --namespace default --resource pods --verb=list,delete,create
kubectl create rolebinding dev-user-bindings --namespace=default --role developer --user=dev-user

or create solution manifest

developer-role-and-binding.yaml


kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io


controlplane ~ ➜  kubectl create -f dev-role-and-bind.yaml 
role.rbac.authorization.k8s.io/developer created
rolebinding.rbac.authorization.k8s.io/dev-user-binding created


A set of new roles and role-bindings are created in the blue namespace for the dev-user. However, the dev-user is unable to get details of the dark-blue-app pod in the blue namespace. Investigate and fix the issue.

We have created the required roles and rolebindings, but something seems to be wrong.


-->  kubectl edit role developer -n blue  --> add resource dark-blue-app


Add a new rule in the existing role developer to grant the dev-user permissions to create deployments in the blue namespace.

Remember to add api group "apps"

 k --as dev-user create deployment ahmed --image ngninx -n blue
error: failed to create deployment: deployments.apps is forbidden: User "dev-user" cannot create resource "deployments" in API group "apps" in the namespace "blue"

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2022-11-09T20:12:02Z"
  name: developer
  namespace: blue
  resourceVersion: "2843"
  uid: 536c4797-3af0-4893-8cef-750e646d9287
rules:
- apiGroups:
  - ""
  resourceNames:
  - dark-blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
- apiGroups:
  - "apps"
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete

----

Cluster Roles

-- namespace or -- cluster scoped 


controlplane $ kubectl api-resources --namespaced=true
NAME                        SHORTNAMES   APIVERSION                     NAMESPACED   KIND
bindings                                 v1                             true         Binding
configmaps                  cm           v1                             true         ConfigMap
endpoints                   ep           v1                             true         Endpoints
events                      ev           v1                             true         Event
limitranges                 limits       v1                             true         LimitRange
persistentvolumeclaims      pvc          v1                             true         PersistentVolumeClaim
pods                        po           v1                             true         Pod
podtemplates                             v1                             true         PodTemplate
replicationcontrollers      rc           v1                             true         ReplicationController
resourcequotas              quota        v1                             true         ResourceQuota
secrets                                  v1                             true         Secret
serviceaccounts             sa           v1                             true         ServiceAccount
services                    svc          v1                             true         Service
controllerrevisions                      apps/v1                        true         ControllerRevision
daemonsets                  ds           apps/v1                        true         DaemonSet
deployments                 deploy       apps/v1                        true         Deployment
replicasets                 rs           apps/v1                        true         ReplicaSet
statefulsets                sts          apps/v1                        true         StatefulSet
localsubjectaccessreviews                authorization.k8s.io/v1        true         LocalSubjectAccessReview
horizontalpodautoscalers    hpa          autoscaling/v2                 true         HorizontalPodAutoscaler
cronjobs                    cj           batch/v1                       true         CronJob
jobs                                     batch/v1                       true         Job
leases                                   coordination.k8s.io/v1         true         Lease
networkpolicies                          crd.projectcalico.org/v1       true         NetworkPolicy
networksets                              crd.projectcalico.org/v1       true         NetworkSet
endpointslices                           discovery.k8s.io/v1            true         EndpointSlice
events                      ev           events.k8s.io/v1               true         Event
ingresses                   ing          networking.k8s.io/v1           true         Ingress
networkpolicies             netpol       networking.k8s.io/v1           true         NetworkPolicy
poddisruptionbudgets        pdb          policy/v1                      true         PodDisruptionBudget
rolebindings                             rbac.authorization.k8s.io/v1   true         RoleBinding
roles                                    rbac.authorization.k8s.io/v1   true         Role
csistoragecapacities                     storage.k8s.io/v1              true         CSIStorageCapacity



controlplane $ kubectl api-resources --namespaced=false
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
componentstatuses                 cs           v1                                     false        ComponentStatus
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumes                 pv           v1                                     false        PersistentVolume
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
bgpconfigurations                              crd.projectcalico.org/v1               false        BGPConfiguration
bgppeers                                       crd.projectcalico.org/v1               false        BGPPeer
blockaffinities                                crd.projectcalico.org/v1               false        BlockAffinity
caliconodestatuses                             crd.projectcalico.org/v1               false        CalicoNodeStatus
clusterinformations                            crd.projectcalico.org/v1               false        ClusterInformation
felixconfigurations                            crd.projectcalico.org/v1               false        FelixConfiguration
globalnetworkpolicies                          crd.projectcalico.org/v1               false        GlobalNetworkPolicy
globalnetworksets                              crd.projectcalico.org/v1               false        GlobalNetworkSet
hostendpoints                                  crd.projectcalico.org/v1               false        HostEndpoint
ipamblocks                                     crd.projectcalico.org/v1               false        IPAMBlock
ipamconfigs                                    crd.projectcalico.org/v1               false        IPAMConfig
ipamhandles                                    crd.projectcalico.org/v1               false        IPAMHandle
ippools                                        crd.projectcalico.org/v1               false        IPPool
ipreservations                                 crd.projectcalico.org/v1               false        IPReservation
kubecontrollersconfigurations                  crd.projectcalico.org/v1               false        KubeControllersConfiguration
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment



- cluster Admin , storage Admin 


cluster-admin-role.yam

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list","create","delete"]


cluster-admin-role-bind.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
  namespace: development
subjects:
- kind: User
  name: cluster-admin 
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io

cluster role are wide named and they are not part of any namespace 



A new user michelle joined the team. She will be focusing on the nodes in the cluster. Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes.


---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io


controlplane ~ ➜  kubectl create -f michelle-cluster-role-bind.yaml 
clusterrole.rbac.authorization.k8s.io/node-admin created
clusterrolebinding.rbac.authorization.k8s.io/michelle-binding created


michelle's responsibilities are growing and now she will be responsible for storage as well. Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage.

Get the API groups and resource names from command kubectl api-resources. Use the given spec:

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]
  
---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io


controlplane ~ ➜  kubectl create -f michelle-storage-role-bind.yaml 
clusterrole.rbac.authorization.k8s.io/storage-admin created
clusterrolebinding.rbac.authorization.k8s.io/michelle-storage-admin created

-------------------

users
serviceaccount


kubectl create serviceaccount dashboard-sa
kubectl get serviceaccount
kubectl describe serviceaccount dashboard-sa --token 
--> creates a secret object, stores token in that object then links service account

kubectl describe secret dashboard-sa-token-kddbm 

curl https://ip_address:6443/api -insecure --header "Authorization: Bearer 4rjdjdhf" 


/var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv

kubectl exec -it my-k8s-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount

ca.crt  namepsace token

cant edit service account for a pod - you will have to delete and re-create pod
but we can do that for a deployment. 


kubectl create serviceaccount dashboard-sa

Enter the access token in the UI of the dashboard application. Click Load Dashboard button to load Dashboard


Create an authorization token for the newly created service account, copy the generated token and paste it into the token field of the UI.

To do this, run kubectl create token dashboard-sa for the dashboard-sa service account, copy the token and paste it in the UI.

-------------

image security 

image: docker.io/library/nginx
	reg     /user/   image


docker login private-registry.io

docker run private-registry.io/apps/internal-app

kubectl create secret docker-registry regcred \
--docker-server= private.registry.io \
--docker-username= registry-user \
--docker-password= registry-password \
--docker-email= registry-user@org.com


apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: redis
    type: backend
spec:
  containers:
  - name: redis
    image: redis
  imagePullSecrets:
  - name: regcred  


root@controlplane:/# kubectl create secret --help
Create a secret using specified subcommand.

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory, or literal value
  tls             Create a TLS secret


kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-email=dock_user@myprivateregistry.com --docker-server=myprivateregistry.com:5000

k edit deployment web

put imagePullSecrets first thing spec 


-----------------------

Storage in docker

/var/lib/docker

storage drivers :  AUFS, ZFS, BTFRS, Device Mapper, Overlay, Overlay2 

volume drivers : local, Azure File Storage, Convoy, DigitalOcean Block storage, Flocker, Ceph

CSI container storage interface

CRI Container runtime interface --> docker, rkt, cri-o

CNI Container network interface

volumes & mounts

apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: redis
    type: backend
spec:
  containers:
    - name: redis
      image: redis
      volumeMounts:
      - mountPath: /opt
        name: data-volume
  volumes:
    - name: data-volume
      hostPath:
          path: /data
          type: Directory 

-----------

PV access modes

- ReadWriteOnce
- ReadOnlyMany
- ReadWriteMany


Configure a volume to store these logs at /var/log/webapp on the host.

Use the spec provided below.

First delete the existing pod by running the following command: -

kubectl delete po webapp
then use the below manifest file to create a webapp pod with given properties as follows:

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory


Persistent Volumes

to be configured in cluster type storage accessible to all k8s cluster
instead of volumes configured in pod definition file

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
 
spec:
  accessModes: 
      - ReadWriteOnce
  capacity:
      storage: 1Gi
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4


kubectl create -f pv-definition.yaml
kubectl get persistentvolume 


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
      - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi


kubectl delete pv myclaim

* what happens when you delete, you can choose to retain with:

PersistentVolumeReclaimPolicy: Retain/Delete/Recycle


Using PVCs in PODs
Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:



apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim


The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.



Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes


Create a Persistent Volume with the given specification.


Volume Name: pv-log

Storage: 100Mi

Access Modes: ReadWriteMany

Host Path: /pv/log

Reclaim Policy: Retain


apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  accessModes:
    - ReadWriteMany
  capacity:
      storage: 100Mi
  hostPath:
    path: /pv/log
  persistentVolumeReclaimPolicy: Retain


Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.

Volume Name: claim-log-1

Storage Request: 50Mi

Access Modes: ReadWriteOnce


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi


controlplane ~ ➜  kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Pending                                                     36s

controlplane ~ ➜  kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-log   100Mi      RWX            Retain           Available                                   6m21s

Why is the claim not bound to the available Persistent Volume?

Access Mode mismatch


Update the Access Mode on the claim to bind it to the PV.

Delete and recreate the claim-log-1.

controlplane ~ ➜  kubectl replace -f pv-log.yaml --force
persistentvolume "pv-log" deleted
persistentvolume/pv-log replaced

controlplane ~ ➜  kubectl get pvc
NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Bound    pv-log   100Mi      RWO                           3m1s


Update the webapp pod to use the persistent volume claim as its storage.

Replace hostPath configured earlier with the newly created PersistentVolumeClaim.


apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
      
      
      
 -----
 
 storage classes --> dynamic provisioning 
 
 controlplane ~ ✖ kubectl describe pv 
Name:              local-pv
Labels:            <none>
Annotations:       <none>
Finalizers:        [kubernetes.io/pv-protection]
StorageClass:      local-storage
Status:            Available
Claim:             
Reclaim Policy:    Retain
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          500Mi
Node Affinity:     
  Required Terms:  
    Term 0:        kubernetes.io/hostname in [controlplane]
Message:           
Source:
    Type:  LocalVolume (a persistent volume backed by local storage on a node)
    Path:  /opt/vol1
Events:    <none>
 
 controlplane ~ ✖ kubectl describe pv 
Name:              local-pv
Labels:            <none>
Annotations:       <none>
Finalizers:        [kubernetes.io/pv-protection]
StorageClass:      local-storage
Status:            Available
Claim:             
Reclaim Policy:    Retain
Access Modes:      RWO
VolumeMode:        Filesystem
Capacity:          500Mi
Node Affinity:     
  Required Terms:  
    Term 0:        kubernetes.io/hostname in [controlplane]
Message:           
Source:
    Type:  LocalVolume (a persistent volume backed by local storage on a node)
    Path:  /opt/vol1
Events:    <none>
 
Create a new PersistentVolumeClaim by the name of local-pvc that should bind to the volume local-pv.


apiVersion : v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
      
      
 Create a new pod called nginx with the image nginx:alpine. The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.

The PV local-pv should in a bound state.

CheckCompleteIncomplete
Pod created with the correct Image?

Pod uses PVC called local-pvc?

local-pv bound?

nginx pod running?

Volume mounted at the correct path?
      
      
      
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc
  
  
  Create a new Storage Class called delayed-volume-sc that makes use of the below specs:

provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

CheckCompleteIncomplete
Storage Class uses: kubernetes.io/no-provisioner ?

Storage Class volumeBindingMode set to WaitForFirstConsumer ?


---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer



-----------------------------------

POD Networking 

LAN -- 192.168.1.0

NODE01 192.168.1.11
NODE02 192.168.1.12
NODE03 192.168.1.13

created bridge in namespace v-net-0
each bridge will have their own ip private ip address

ip addr add 10.244.1.1 dev v-net-0
ip addr add 10.244.2.1 dev v-net-0
ip addr add 10.244.3.1 dev v-net-0

run netscript.sh on every container.



to attach conntainer to network, we need a pipe or a virtual network cable
#create veth pair
1. ip link add ..


#attach veth pair
2. ip link set ..
   ip link set ..
#assign ip address
3. ip -n namespace addr add .. 
   ip -n namespace route add ..
#bring up interface
   ip -n namespace link set ..


now pods have ip addr and can communicate with each other on same node.
now enable routes for each pod to reach pods on other nodes

define routes

node 1--> ip route add 10.244.2.2 via 192.168.1.12
node 1--> ip route add 10.244.3.2 via 192.168.1.13

node 2--> ip route add 10.244.1.2 via 192.168.1.11
node 2 --> ip route add 10.244.3.2 via 192.168.13

node 3--> ip route add 10.244.1.2 via 192.168.1.11
node 3 --> ip route add 10.244.2.2 via 192.168.1.12

to automatically run the netscript on each created container is by container network interface CNI 

in kubelet conf

--cni-conf-dir= /etc/cni/net.d
--cni-bin-dir=/etc/cni/bin
./net-script.sh add container namespace 

CNI IPAM
- host-local
- DHCP 

kube-proxy - ipvs, userspace, iptables(default)

to check cluster ip range --> ps -ef | grep kube-api 
to check pod cluster ip range --> check weave pods logs - kubectl logs weave_pod -n kube-system
to check service cluster ip range --> ps -ef | grep kube-api
Determine kube proxy type by --> kubectl logs kube-proxy-pod -n kube-system
to determine how k8s cluster ensures each node has a proxy-pod, check proxy pods desc --> daemonsets

---------------

DNS 

K8s setups internal DNS by default to resolve pod names, and service names
if you setup K8s manually, u have to set it up yourself

KubeDNS example
two pods and a service
https://github.com/amieltahir
10.244.1.5    test-pod

10.244.2.5     web-pod
10.107.37.188  web-service

when we create a service, DNS creates a record for it now we can access from any pod because they are in same namespace --> curl https://web-service 
if we create a namespace (apps) for web service and web-pod then we can access it --> curl http://web-service.apps

hostname: web-service   
type: svc  
ip_address: 10.107.37.188
namespace: apps
root: cluser.local

curl http://web-service.apps.svc.cluster.local

hostname: 10.244.2.5 
type: pod
namespace: apps
root: cluster.local
ip_address: 10.244.2.5 

hostname: 10.244.1.5
type: pod
namespace: default
root: cluster.local
ip_address: 10.244.1.5

--------

CoreDNS kubernetes

we move all /etc/hosts enrties to centralized DNS server ( 10.96.0.10) and add dns IP address on each pod /etc/resolv.conf  ----> KUBE_DNS
after version 12 we recommend using CoreDNS
We run CoreDNS as a pod on a replica set as part of deployment and its runs core dns executable ./CoreDNS

CoreDNS conf --> /etc/coreDNS/corefile

this file is passed as config map object, if we need to modify it we modify the coredns config map
kubectl get configmap -n kube-system --> coredns

also deploys a kube-dns service, type clusterIP
kubectl get svc -n kube-system 

to get IP configured for CoreDNS check -->  /var/lib/kubelet/config.yaml
to nslookup service from pod -->   kubectl exec -it hr -- nslookup mysql.service > /root/cka/nslooup.out 


















