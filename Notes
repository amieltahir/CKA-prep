sed -i 's/old-text/new-text/g' input.txt

redis.yml

apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: redis
    type: backend
spec:
  containers:
    - name: redis
      image: redis
---------------------------------

cat replicaset-definition-1.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
  labels:
        tier: frontend
spec:
  template:
    metadata:
      name: app1
      labels:
        app: app1
        type: front-end
    spec:
      containers:
      - name: nginx
        image: nginx
  replicas: 2
  selector:
    matchLabels:
      type: front-end

-------------------------------------

 cat replicaset-definition-2.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-2
  labels:
        tier: front-end
spec:
  template:
    metadata:
      name: app2
      labels:
        app: app2
        type: front-end
    spec:
      containers:
      - name: nginx
        image: nginx
  replicas: 2
  selector:
    matchLabels:
      type: front-end


kubectl scale --replicas=2 replicaset new-replica-set 

------------------------

kubectl get all

deployment has same definition as replicaset except for kind. 

Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginxla

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create -f nginx-deployment.yaml

OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

---------------

 cat deployment-definition-1.yaml 

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
  labels:
    app: deployment-1
    type: front-end
spec:
  template:
    metadata:
      name: deployment-1
      labels: 
        app: deployment-1 
        type: front-end
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600
  replicas: 4
  selector:
    matchLabels:
      type: front-end

---------
services:

* node port
* cluster IP
* load balancer

node port

apiVersion: v1
kind: Service
metadata:
    name: myapp-service

spec:
     type: NodePort
     ports:
      - targetPort: 80
        port: 80
        nodePort: 30008

     selector:
        app: myapp
        type: front-end


--------------

it could be three pods on the same node or three pods on three different nodes.

target port of pod :80
port  of service : 80
node port : 30008

-----------------------------------------

Cluster IP - service-definition.yml 

apiVersion: v1
kind: Service
metadata:
    name: back-end
spec:
   type: ClusterIP
   ports:
   - targetPort: 80
     port: 80
   selector:
     app: myapp
     type: back-end

------------------------

Load Balancer - service-definition.yml 

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
spec:
   type: LoadBalancer
   ports:
   - targetPort: 80
     port: 80
     nodePort: 30008

------------------------------

creating a node port service

controlplane ~ ➜  cat service-definition-1.yaml 

---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp

--------------------

namespaces 

namespace-dev.yml

apiVersion: v1
kind: Namespace
metadata: 
    name: dev

--------------------------
kubectl create -f namespace-dev.yml
kubectl create namespace dev

kubectl config set-context $(kubectl config current-context) --namespace dev
kubectl get pods --all-namespaces 

--------------------------------

Resource quota

apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

---------

Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port.
 I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

Reference:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

https://kubernetes.io/docs/reference/kubectl/conventions/=


--------------------


kubectl get pods --selector=tier=frontend --selector=bu
kubectl get pods --selector env=prod,bu=finance,tier=frontend


controlplane ~ ➜  cat replicaset-definition-1.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: front-end
   template:
     metadata:
       labels:
        tier: front-end 
     spec:
       containers:
       - name: nginx
         image: nginx

------

taints and tolerations

kubectl taint nodes node-name key=value:taint-effect [noSchedule,PreferNoSchedule,NoExecute]

pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: app
    operator: "Equal"
    value: "Blue"
    effect: "NoSchedule"


kubectl describe node kubemaster | grep Taint
kubectl taint nodes node01 spray=mortein:NoSchedul



bee.yaml

controlplane ~ ➜  cat bee.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  tolerations: 
  - key: spray
    operator: "Equal"
    value: "mortein"
    effect: "NoSchedule"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

------


add a hyphen to remove taint from node
kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

-------------

node selector 

kubectl label nodes node-name label-key=label-value
kubectl label nodes node-1 size=Large

its limited so we use node affinity

node affinity types

Available
1.requiredDuringSchedulingIgnoredDuringExecution
2.preferredDuringSchedulingIgnoredDuringExecution

Planned:
1.requiredDuringSchedulingRequiredDuringExecution



Name: blue

Replicas: 3

Image: nginx

NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

Key: color

value: blue



spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue



Name: red

Replicas: 2

Image: nginx

NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

Key: node-role.kubernetes.io/control-plane

Use the right operator


spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - controlplane

----------


n the previous lecture, I said - "When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi".
 For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.



apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/



apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/



References:

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource

-----

practice memory and cpu limits on 
https://killercoda.com/playgrounds/scenario/cka
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

kubectl create namespace default-cpu-example

Limit Range example


apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container


kubectl create -f limit-range.yaml --namespace=default-cpu-example
kubectl get limitrange -n default-cpu-example

Now if you create a Pod in the default-cpu-example namespace, and any container in that Pod does not specify its own values for CPU request and CPU limit,
 then the control plane applies default values: a CPU request of 0.5 and a default CPU limit of 1.

Here's a manifest for a Pod that has one container. The container does not specify a CPU request and limit.





cpu-defauls-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx


kubectl create -f cpu-default-pod.yaml --namespace default-cpu-example
kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example

The output shows that the Pod's only container has a CPU request of 500m cpu (which you can read as “500 millicpu”), and a CPU limit of 1 cpu.
 These are the default values specified by the LimitRange.


What if you specify a container's limit, but not its request? 

cpu-default-pod-2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-2
spec:
  containers:
  - name: default-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "1"


The output shows that the container's CPU request is set to match its CPU limit. Notice that the container was not assigned the default CPU request value of 0.5 cpu:

--> 

resources:
      limits:
        cpu: "1"
      requests:
        cpu: "1"


What if you specify a container's request, but not its limit?

cpu-default-pod-3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-3
spec:
  containers:
  - name: default-cpu-demo-3-ctr
    image: nginx
    resources:
      requests:
        cpu: "0.75"

The output shows that the container's CPU request is set to the value you specified at the time you created the Pod (in other words: it matches the manifest). 
However, the same container's CPU limit is set to 1 cpu, which is the default CPU limit for that namespace.

resources:
  limits:
    cpu: "1"
  requests:
    cpu: 750m

----------------------------------------------------

kubectl create namespace default-mem-example


memory-default.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

kubectl create -f memory-default.yaml 


Now if you create a Pod in the default-mem-example namespace, and any container within that Pod does not specify its own values for memory request and memory limit,
then the control plane applies default values: a memory request of 256MiB and a memory limit of 512MiB.

apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo
spec:
  containers:
  - name: default-mem-demo-ctr
    image: nginx

kubectl create -f default-memory-pod.yaml

kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example

The output shows that the Pod's container has a memory request of 256 MiB and a memory limit of 512 MiB. These are the default values specified by the LimitRange.

-->

resources:
      limits:
        memory: 512Mi
      requests:
        memory: 256Mi


What if you specify a container's limit, but not its request?

default-memory-pod-2.yaml


apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-2
spec:
  containers:
  - name: default-mem-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "1Gi"


kubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example


The output shows that the container's memory request is set to match its memory limit. 
Notice that the container was not assigned the default memory request value of 256Mi.

-->

resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi

----------------------

What if you specify a container's request, but not its limit?

default-memory-pod-3.yaml

apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-3
spec:
  containers:
  - name: default-mem-demo-3-ctr
    image: nginx
    resources:
      requests:

        memory: "128Mi"


The output shows that the container's memory request is set to the value specified in the container's manifest. The container is limited to use no more than 512MiB of memory, which matches the default memory limit for the namespace.

resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi

--------------------------------

Daemon Sets -- 


apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
     matchLabels:
	app: monitoring-agent
  template:
     metadata:
       labels: 
         app: monitoring-agent
     spec:
       containers:
       - name: monitoring-agent
         image: monitoring-agent


Deploy a DaemonSet for FluentD Logging.

Use the given specifications.

CheckCompleteIncomplete
Name: elasticsearch

Namespace: kube-system

Image: k8s.gcr.io/fluentd-elasticsearch:1.20


An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command 
kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml. 
Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet.

Finally, create the Daemonset by running kubectl create -f fluentd.yaml


controlplane ~ ➜  cat fluentd.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  creationTimestamp: null
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: k8s.gcr.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
        resources: {}

-------------------------

Static pods

kubelet -->  /etc/kubernetes/manifests
kubelet.service --> --pod-manifest-path = /etc/kubernetes/manfiests
or in  		    --config=kubeconfig.yaml    -->  inside this file --> staticPodPath: /etc/kubernetes/manifests
 
you cant modify static pod while running you have to edit manifest file

Static Pods Vs Daemon Sets

Static Pods --> created by Kubelet.   --> Deploy Control Plane Components as Static Pods
DaemonSets --> Created by Kube-API-Server ( DaemonSet Controller) --> Deploy Monitoring Agents, Logging Agents on nodes

Both ignored by Kube Scheduler. 

----------------------------

Find static pods

Run the command kubectl get pods --all-namespaces and look for those with -controlplane appended in the name

Run the kubectl get pods --all-namespaces -o wide and identify the node in which static pods are deployed.

Create a static pod named static-busybox that uses the busybox image and the command sleep 1000 

-->   Create a pod definition file called static-busybox.yaml with the provided specs and place it under /etc/kubernetes/manifests directory

kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml


edit to busybox:1.28.4

Simply edit the static pod definition file and save it. If that does not re-create the pod, run: kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

---------------------------

multiple schedulers

- default scheduler 

my-scheduler-config.yaml

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler


kubectl get events -o wide
kubectl logs my-custom-scheduler --name-space=kube-system


We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.


Checkout the following Kubernetes objects:

ServiceAccount: my-scheduler (kube-system namespace)
ClusterRoleBinding: my-scheduler-as-kube-scheduler
ClusterRoleBinding: my-scheduler-as-volume-scheduler

Run the command: kubectl get serviceaccount -n kube-system & kubectl get clusterrolebinding

Note: - Don't worry if you are not familiar with these resources. We will cover it later o


Let's create a configmap that the new scheduler will employ using the concept of ConfigMap as a volume.
Create a configmap with name my-scheduler-config using the content of file /root/my-scheduler-config.yaml

kubectl create cm my-scheduler-config -n kube-system --from-file /root/my-scheduler-config.yaml 



Deploy an additional scheduler to the cluster following the given specification.

Use the manifest file provided at /root/my-scheduler.yaml. Use the same image as used by the default kubernetes scheduler  (k8s.gcr.io/kube-scheduler:v1.24.0)

--> modify yaml file with correct image then

kubectl create -f /root/my-scheduler -n kube-system


A POD definition file is given. Use it to create a POD with the new custom scheduler.

File is located at /root/nginx-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  schedulerName: my-scheduler
                       

-----------------------------------------------

Scheduling profiles

pod-definition.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  priorityClassName: high-priority 
  containers: 
  - name: simple-webapp-color
    image: simple-webapp-color
    resources:
      requests:
        memory: "1Gi"
	cpu: 10


apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 10000000
globalDefault: false
description: " This priority class should be used for xyz pods only"


1. scheduling queue [ PrioritySort] 
2. filtering [NodeResourcesFit][NodeName][NodeUnschedulable]
3. scoring [NodeResourcesFit][ImageLocality]
4. binding [DefaultBinder]

* Extension Points [QueueSort, PreFilter, Filter, PostFilter,PreScore,score,reserve,permit,preBind,bind,postBind]

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler
   plugins:
     score:
        disabled:
         - name: TaintToleration
        enabled:
         - name: MyCustomPluginA
         - name: MyCustomPluginB


apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
 - schedulerName: my-scheduler-2
   plugins:
    preScore:
      disabled:
       - name: '*'
    score:
      enabled:
       - name: '*' 


https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md



https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/



https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/



https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

-------------------------------------

kubectl logs -f pod_name

------------------------

Application life cycle

kubectl create -f deployment-def.yaml
kubectl get deployments
kubeclt apply -f deployment-def.yaml
kubectl set image deployment/myapp-deployment nginx:1.9.1
kubectl rollout status deployment/myapp-deployment
kubectl history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

------

commands and arguments


apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: debian
    command: ["printenv"]
    args: ["HOSTNAME", "KUBERNETES_PORT"]
  restartPolicy: OnFailure

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
    args:
      - "1200"


---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"



Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]

-------------------------------------------

Configuring ENV variables in applications

  env: 
   
    - name: APP_COLOR
      value: pink
   
  env: 
   
    - name: APP_COLOR
      valueFrom:
          configMapKeyRef:
  
  env: 
   
    - name: APP_COLOR
      valueFrom:
          secretKeyRef:

----------------------------------


Create config maps

1.create cm
2. inject

imperative 

kubectl create configmap config-name --from-literal=key=value
kubectl create configmap app-config --from-literal=APP_COLOR=blue
or
kubectl create configmap app-config --from-file=app_config.properties

declartive

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR=blue
  APP_MODE= prod

kubectl create -f config-map.yaml

kubectl get configmaps
kubectl describe configmaps


pod injection 

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    envFrom:
      - configMapRef:
	     name: app-config
     or
   
  
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    env:
      - name: APP_COLOR
         configMapKeyRef:
           name: app-config
           key: APP_COLOR


or
   volumes:
   - name: app-config-volume
     configMap:
       name: app-config


---------------

create secrets then inject into pods

1.imperative.

kubectl create secret generic <secret-name> --from-literal=key=value
kubectl create secret generic app-secret --from-literal=DB_HOST=mysql --from-literal=DB_USER=root --from-literal=DB_PASSWD=passwd

kubectl create secret generic app-secret --from-file=app_secret.properties.

2. declarative


apiVersion: v1
kind: Secret
metadata:  
  name: app-secret
data:
  DB_HOST= mysql
  DB_USER= root
  DB_PASS= passwd 

encode strings in secret

echo -n 'mysql' | base64
echo -n 'root'  | base64
echo -n 'passwd' | base64

kubectl get secrets
kubectl describe secret
kubectl get app-secret -o YAML


kubectl create -f secret-data.yaml 

-----------------------
 Secrets in Pods

ENV


apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    ports:
      - containerPort: 8080
    envFrom:
      secretRef:
        name: app-secret


SINGLE ENV
   
    env:
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: app-secret  
	    key: DB_Password


Secrets in Pods as Volumes

     volumes:
     - name: app-secret-volume
       secret:
	 secretName: app-secret


Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 



Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here



Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.

kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
-----------------

---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
--------------

Encryption at rest


kubectl create secret generic my-secret --from-literal=key1=supersecret
k describe secret my-secret
k get secret my-secret -o YAML

apiVersion: v1
data:
  key1: c3VwZXJzZWNyZXQ=
kind: Secret
metadata:
  creationTimestamp: "2022-10-27T23:24:49Z"
  name: my-secret
  namespace: default
  resourceVersion: "4478"
  uid: fae657ec-9cf0-493d-85bb-6870075a6d0e
type: Opaque


echo "c3VwZXJzZWNyZXQ=" | base64 --decode  ---> supersecret


if etcdctl is not there --> apt-get install etcd-client


kubectl get pod -n kube-system --> etcd-controlplane 

ls -ltr /etc/kubernetes/pki/etcd/ca.crt
ls -ltr /etc/kubernetes/pki/etcd/

ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/my-secret | hexdump -C


-->

00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 00  0a 0c 0a 02 76 31 12 06  |ret.k8s.....v1..|
00000030  53 65 63 72 65 74 12 d0  01 0a b0 01 0a 09 6d 79  |Secret........my|
00000040  2d 73 65 63 72 65 74 12  00 1a 07 64 65 66 61 75  |-secret....defau|
00000050  6c 74 22 00 2a 24 66 61  65 36 35 37 65 63 2d 39  |lt".*$fae657ec-9|
00000060  63 66 30 2d 34 39 33 64  2d 38 35 62 62 2d 36 38  |cf0-493d-85bb-68|
00000070  37 30 30 37 35 61 36 64  30 65 32 00 38 00 42 08  |70075a6d0e2.8.B.|
00000080  08 c1 a6 ec 9a 06 10 00  8a 01 61 0a 0e 6b 75 62  |..........a..kub|
00000090  65 63 74 6c 2d 63 72 65  61 74 65 12 06 55 70 64  |ectl-create..Upd|
000000a0  61 74 65 1a 02 76 31 22  08 08 c1 a6 ec 9a 06 10  |ate..v1"........|
000000b0  00 32 08 46 69 65 6c 64  73 56 31 3a 2d 0a 2b 7b  |.2.FieldsV1:-.+{|
000000c0  22 66 3a 64 61 74 61 22  3a 7b 22 2e 22 3a 7b 7d  |"f:data":{".":{}|
000000d0  2c 22 66 3a 6b 65 79 31  22 3a 7b 7d 7d 2c 22 66  |,"f:key1":{}},"f|
000000e0  3a 74 79 70 65 22 3a 7b  7d 7d 42 00 12 13 0a 04  |:type":{}}B.....|
000000f0  6b 65 79 31 12 0b 73 75  70 65 72 73 65 63 72 65  |key1..supersecre|   -->> secret not encrypted at rest
00000100  74 1a 06 4f 70 61 71 75  65 1a 00 22 00 0a        |t..Opaque.."..|
0000010e

ensure that encryption is enabled ( its not yet)

ps -aux | grep kube-api | grep encryption-provider-config

cat /etc/kubernetes/manifests/kube-api.yaml --> not encryption provider in contanier commands 

create new encryption config file

create random key -->  head -c 32 /dev/urandom | base64  --> Jwg11KNvQ3tjQR+NyevAtoHipbKtve7oymbnJfb1jVA=

vi enc.yml

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: Jwg11KNvQ3tjQR+NyevAtoHipbKtve7oymbnJfb1jVA=
      - identity: {}


add this line to kube-api -->  --encryption-provider-config=/etc/kubernetes/enc/enc.yaml
also add below volume mount

- name: enc                          
  mountPath: /etc/kubernetes/enc      
  readonly: true 

also add below volume

- name: enc                             
  hostPath:                             
  path: /etc/kubernetes/enc           
  type: DirectoryOrCreate    


save file then get pods or crictl pods

now its using encryption

controlplane $ ps -aux | grep kube-api | grep encryption
root       82736  5.6 16.9 1111876 345680 ?      Ssl  00:15   0:05 kube-apiserver --advertise-address=172.30.1.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --encryption-provider-config=/etc/kubernetes/enc/enc.yaml

mkdir /etc/kubernetes/enc 
mv enc.yaml /etc/kubernetes/enc 


create another secret 

kubectl create secret generic my-secret-2 --from-literal=key2=supersecret

ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/my-secret-2 | hexdump -C

encryption enabled -->


00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 2d 32 0a 6b 38  73 3a 65 6e 63 3a 61 65  |ret-2.k8s:enc:ae|
00000030  73 63 62 63 3a 76 31 3a  6b 65 79 31 3a c8 b7 e5  |scbc:v1:key1:...|
00000040  b0 5f b2 a9 52 77 2e 38  23 a0 d7 d5 43 32 10 fe  |._..Rw.8#...C2..|
00000050  42 3a 19 d0 b5 a6 b4 1c  46 3e 47 ad 4e 6b ea fc  |B:......F>G.Nk..|
00000060  ed aa d5 a7 57 85 d0 94  56 83 f6 5b 16 30 b0 13  |....W...V..[.0..|
00000070  46 04 f7 25 d1 68 4c 09  67 6a 3d 10 86 c6 e4 9c  |F..%.hL.gj=.....|
00000080  da ef 0d 19 a5 d9 e4 6b  ce 0b 1c ce 7f 58 6c 25  |.......k.....Xl%|
00000090  c3 ac bd fe 08 a9 d0 3d  70 3b e0 32 a3 ae 76 13  |.......=p;.2..v.|
000000a0  0f 5c af 25 0d 61 43 a9  f4 b5 f3 38 ce cf 10 5b  |.\.%.aC....8...[|
000000b0  af 78 1b fd f9 fd 9c 66  50 b4 1a 40 ce bc 68 cc  |.x.....fP..@..h.|
000000c0  1a 48 04 4f ca 70 d6 6f  c2 75 4e 5e 39 d6 47 9c  |.H.O.p.o.uN^9.G.|
000000d0  d8 64 3e 46 c1 37 21 c4  3b be f3 51 84 f1 1a 58  |.d>F.7!.;..Q...X|
000000e0  93 e7 9d 33 30 ef 9c 15  a4 08 e0 c2 f6 5e 5d 57  |...30........^]W|
000000f0  fe a4 5f aa a8 bc 9f 4b  31 38 f9 47 53 a6 c9 66  |.._....K18.GS..f|
00000100  22 dc 2f f1 80 2b a0 fc  65 89 0e 0d c3 77 ef 68  |"./..+..e....w.h|
00000110  32 68 17 33 8c 52 6f 0e  14 6b 9a 4b d6 98 0a f2  |2h.3.Ro..k.K....|
00000120  84 27 0e 92 7d d1 90 62  b7 d3 ff 77 cc 46 fc 4f  |.'..}..b...w.F.O|
00000130  94 8c 4d 19 8b 4e e0 05  9d 31 3c 16 44 0a        |..M..N...1<.D.|
0000013e

old one still not encrypted -->

0000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 00  0a 0c 0a 02 76 31 12 06  |ret.k8s.....v1..|
00000030  53 65 63 72 65 74 12 d0  01 0a b0 01 0a 09 6d 79  |Secret........my|
00000040  2d 73 65 63 72 65 74 12  00 1a 07 64 65 66 61 75  |-secret....defau|
00000050  6c 74 22 00 2a 24 66 61  65 36 35 37 65 63 2d 39  |lt".*$fae657ec-9|
00000060  63 66 30 2d 34 39 33 64  2d 38 35 62 62 2d 36 38  |cf0-493d-85bb-68|
00000070  37 30 30 37 35 61 36 64  30 65 32 00 38 00 42 08  |70075a6d0e2.8.B.|
00000080  08 c1 a6 ec 9a 06 10 00  8a 01 61 0a 0e 6b 75 62  |..........a..kub|
00000090  65 63 74 6c 2d 63 72 65  61 74 65 12 06 55 70 64  |ectl-create..Upd|
000000a0  61 74 65 1a 02 76 31 22  08 08 c1 a6 ec 9a 06 10  |ate..v1"........|
000000b0  00 32 08 46 69 65 6c 64  73 56 31 3a 2d 0a 2b 7b  |.2.FieldsV1:-.+{|
000000c0  22 66 3a 64 61 74 61 22  3a 7b 22 2e 22 3a 7b 7d  |"f:data":{".":{}|
000000d0  2c 22 66 3a 6b 65 79 31  22 3a 7b 7d 7d 2c 22 66  |,"f:key1":{}},"f|
000000e0  3a 74 79 70 65 22 3a 7b  7d 7d 42 00 12 13 0a 04  |:type":{}}B.....|
000000f0  6b 65 79 31 12 0b 73 75  70 65 72 73 65 63 72 65  |key1..supersecre|
00000100  74 1a 06 4f 70 61 71 75  65 1a 00 22 00 0a        |t..Opaque.."..|
0000010e


but

Since Secrets are encrypted on write, performing an update on a Secret will encrypt that content.

kubectl get secrets --all-namespaces -o json | kubectl replace -f -

secret/my-secret replaced
secret/my-secret-2 replaced
secret/bootstrap-token-metz81 replaced

now both are encrypted

00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6d 79 2d 73 65 63  |s/default/my-sec|
00000020  72 65 74 0a 6b 38 73 3a  65 6e 63 3a 61 65 73 63  |ret.k8s:enc:aesc|
00000030  62 63 3a 76 31 3a 6b 65  79 31 3a 3c 1d d7 b1 54  |bc:v1:key1:<...T|
00000040  11 82 09 de 1a 5b e5 e1  4c 40 8b ce 95 d3 45 d9  |.....[..L@....E.|
00000050  40 68 26 b8 d2 ca ae 6d  33 20 15 e6 4d 80 d1 94  |@h&....m3 ..M...|
00000060  30 d8 b7 3b 05 39 68 cb  88 a4 01 34 fa 88 b3 77  |0..;.9h....4...w|
00000070  08 5a a2 5f eb 5b 45 03  49 56 14 82 d3 36 9d d7  |.Z._.[E.IV...6..|
00000080  ea 41 62 7c 1a 1b c0 13  0f 9d 64 89 93 67 f2 0f  |.Ab|......d..g..|
00000090  57 35 b5 70 fb af 9a 90  8e ff 72 35 93 77 68 3d  |W5.p......r5.wh=|
000000a0  61 de e5 ad c6 02 1b 20  ca 41 e2 19 fb 41 60 f9  |a...... .A...A`.|
000000b0  70 45 80 9e be 11 f1 0a  8e a1 8e 5c ed b6 dd 54  |pE.........\...T|
000000c0  58 09 d2 ac 51 1b 77 f9  a6 3f 16 c6 85 0f f0 c1  |X...Q.w..?......|
000000d0  b1 e0 78 db ad 2d ed f3  d1 ae 9d ac 8f c6 5a 39  |..x..-........Z9|
000000e0  bf 55 f6 5d a9 9a 9e ef  08 d9 05 0c e6 d1 01 4b  |.U.]...........K|
000000f0  1a 7a 2a e4 ee 70 96 95  b7 74 06 be b1 d2 4e 14  |.z*..p...t....N.|
00000100  53 9a e2 2c 7c a8 ec 8c  5f 70 2d 1d 62 65 0f 7f  |S..,|..._p-.be..|
00000110  e7 fc 66 87 74 e8 41 6f  71 09 73 b2 07 ee be 53  |..f.t.Aoq.s....S|
00000120  5b 52 1e f7 9a e2 43 71  aa 4a 01 fa 16 0e 64 22  |[R....Cq.J....d"|
00000130  c8 ec 75 92 d1 8a 7e 7d  09 c5 30 0a              |..u...~}..0.|
0000013c
